#Data quality Study Protocol

This is an informatics study that focuses on data quality (rather than a clinical question).

#Introduction

Data of sufficient quality is an important pre-requisite for conducting high quality research. In a prior study (Achilles Heel Evaluation), we have conducted an initial proof of concept comparison of using Achilles and Achilles Heel tools across sites. Thanks to this study and additional input on data quality measures, we have implemented new measures and new data quality rules in the latest version of Achilles. (version 1.3 and later). 

The present study (Data Quality study) aims to (1) evaluate new rules; (2) test a restricted subset of Achilles outputs that can be compared accross sites; and (3) decide thresholds for new rules that investigate data density (for a general population datasets as well as specialized population datasets)

#Methods
Each site will be assigned a meaningless identifier and selected Achilles analyses (or measures) will be compared.

##New rules example
For example, Achilles 1.3 contains a rule that looks at count of distinct units for each measurement_concept_id (rule_id 35). This rule currently has "hard coded" thresholds (rule triggers a notification if 10+ measurments have 5+ units). We hope to study the variability of sites and decide the most appropriate threshold for issuing WARNING or NOTIFICATION type of Achilles Heel output for this problem.

Another example is a rule looking at percentage of measurement that have NULL recorded as their numeric value (rule_id 28). This value can often differ radically in claim datasets versus EHR datasets. Empirical study of variability is needed to decide an appropriate threshold.

##Data density measures
Achilles currently computes several useful data density measures that count number of distinct concept_ids per person (e.g., analysis_id's 203 (visits), 403 (condition) or 603 (procedures)). The study is piloting direct use of such data density measures for assesing data quality. It also hopes to explore most appropriate new data quality rules that analyze data density.

##Data requirements
Each site can inspect the CSV (and graph) files generated by the study prior submitting the study for comparative analysis by the study team (comparison of sites to each other). Only aggregated data is being submitted for any analysis. 

An Amazon S3 cloud mechanism (study specific bucket) is used to ensure that a site can contribute data, but not read data of other sites. Only the study analysis team has access to data from all sites for the purposes of the analysis.


#Use of output data
We plan to compare several sites in terms of data quality outputs generated by Achilles and Achilles Heel. The final manuscript about the study results will limit as much as possible what is revealed about each site. Each site will have a chance to review (and edit) the manuscript prior submission to the journal.

If you share your site's data with the DataQuality study principal investigator or the study team, it will be only for the purpose of the study and comparison within the study. All compared sites will be refered to under meaningless site ID. All results will be pooled together so that any site or dataset will be hidden in a crowd of several sites/datasets.

This principle was used in the prior Achilles Heel evaluation study (precursor to this study). If any site requires a formal Data Use Agreement between the your site and the Data Quality Study Principal Investigator, please indicate so. 

